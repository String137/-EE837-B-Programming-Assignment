{"cells":[{"cell_type":"markdown","metadata":{"id":"WTUYbQLlYUQ-"},"source":["# EE837B Advances in Convolutional Neural Networks (Fall 2023)\n","## Programming Assignment\n","Department of Electrical Engineering, KAIST\n","\n","- Course Instructor : Prof. Junmo Kim\n","\n","- Primary TA : Hyounguk Shon\n","\n","- For questions regarding this assignment, use the course Q&A board on KLMS.\n","\n","---\n","\n","In this programming assignment, you are asked to reproduce the [Knowledge Distillation (KD) algorithm](https://arxiv.org/abs/1503.02531) in PyTorch. Your python code should distill a pre-trained teacher model into a smaller student model using the CIFAR-100 dataset. Additionally, we have an optional challenge for bonus credits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxMR39kBUTnQ"},"outputs":[],"source":["import os\n","import random\n","\n","import gdown\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","\n","\n","# set random seed for reproducibility\n","torch.manual_seed(1234)\n","random.seed(1234)\n","np.random.seed(1234)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3729,"status":"ok","timestamp":1701262218552,"user":{"displayName":"이호준","userId":"02920364108939530246"},"user_tz":-540},"id":"JE5e1tq5EXfJ","outputId":"6b42bbe3-92e5-4da5-95c8-9e1eb03082ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1701262218552,"user":{"displayName":"이호준","userId":"02920364108939530246"},"user_tz":-540},"id":"kgj5pnSmEbpD","outputId":"3c734c7f-f9ba-4741-8bf6-60d57898363f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Advanced CNN/For submission\n"]}],"source":["# cd /content/drive/MyDrive/Advanced\\ CNN/For\\ submission"]},{"cell_type":"markdown","metadata":{"id":"GYfj23oOmOr6"},"source":["# Define ResNet architecture\n","Do not change this code block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQdqM8z8ZM6l"},"outputs":[],"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(\n","        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n","    )\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n","        super(BasicBlock, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, depth, num_filters, block_name=\"BasicBlock\", num_classes=100):\n","        super(ResNet, self).__init__()\n","        assert (\n","            depth - 2\n","        ) % 6 == 0, \"When use basicblock, depth should be 6n+2, e.g. 20, 32, 44, 56, 110, 1202\"\n","        n = (depth - 2) // 6\n","        block = BasicBlock\n","\n","        self.inplanes = num_filters[0]\n","        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=3, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(num_filters[0])\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(block, num_filters[1], n)\n","        self.layer2 = self._make_layer(block, num_filters[2], n, stride=2)\n","        self.layer3 = self._make_layer(block, num_filters[3], n, stride=2)\n","        self.avgpool = nn.AvgPool2d(8)\n","        self.fc = nn.Linear(num_filters[3] * block.expansion, num_classes)\n","        self.stage_channels = num_filters\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(\n","                    self.inplanes,\n","                    planes * block.expansion,\n","                    kernel_size=1,\n","                    stride=stride,\n","                    bias=False,\n","                ),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = list([])\n","        layers.append(\n","            block(self.inplanes, planes, stride, downsample, is_last=(blocks == 1))\n","        )\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, is_last=(i == blocks - 1)))\n","\n","        return nn.Sequential(*layers)\n","\n","    def get_feat_modules(self):\n","        feat_m = nn.ModuleList([])\n","        feat_m.append(self.conv1)\n","        feat_m.append(self.bn1)\n","        feat_m.append(self.relu)\n","        feat_m.append(self.layer1)\n","        feat_m.append(self.layer2)\n","        feat_m.append(self.layer3)\n","        return feat_m\n","\n","    def get_bn_before_relu(self):\n","        bn1 = self.layer1[-1].bn2\n","        bn2 = self.layer2[-1].bn2\n","        bn3 = self.layer3[-1].bn2\n","        return [bn1, bn2, bn3]\n","\n","    def get_stage_channels(self):\n","        return self.stage_channels\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)  # 32x32\n","        f0 = x\n","\n","        x, f1_pre = self.layer1(x)  # 32x32\n","        f1 = x\n","        x, f2_pre = self.layer2(x)  # 16x16\n","        f2 = x\n","        x, f3_pre = self.layer3(x)  # 8x8\n","        f3 = x\n","\n","        x = self.avgpool(x)\n","        avg = x.reshape(x.size(0), -1)\n","        out = self.fc(avg)\n","\n","        return out\n","\n","def resnet8x4(**kwargs):\n","    return ResNet(8, [32, 64, 128, 256], \"basicblock\", **kwargs)\n","\n","def resnet32x4(**kwargs):\n","    return ResNet(32, [32, 64, 128, 256], \"basicblock\", **kwargs)"]},{"cell_type":"markdown","metadata":{"id":"YTSJCzWzIgYt"},"source":["# Define Base Trainer Class\n","Do not change this code block.\n","\n","The base trainer will automatically download and load pre-trained teacher weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-ukFLINYURx"},"outputs":[],"source":["class BaseTrainer:\n","    pretrained_teacher_link = 'https://drive.google.com/uc?id=1Gh3Z8BZ62PGD7PQiFiwmU9vMwMpF5F46'\n","\n","    def __init__(self):\n","        self.teacher = resnet32x4(num_classes=100)\n","        self.student = resnet8x4(num_classes=100)\n","        gdown.download(self.pretrained_teacher_link, './resnet_32x4.pth', resume=True)\n","        self.teacher.load_state_dict(torch.load(\"./resnet_32x4.pth\", map_location=\"cpu\")[\"model\"])\n","\n","        self.train_transform = transforms.Compose([\n","                transforms.RandomCrop(32, padding=4),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","                ])\n","        self.test_transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","                ])\n","\n","        self.train_set = datasets.CIFAR100('./data/', download=True, train=True, transform=self.train_transform)\n","        self.test_set = datasets.CIFAR100('./data/', download=False, train=False, transform=self.test_transform)\n","        self.test_dataloader = DataLoader(self.test_set, batch_size=64, shuffle=False)\n","\n","    def save_student_checkpoint(self, ckpt_path):\n","        state_dict = self.student.state_dict()\n","        torch.save(state_dict, ckpt_path)\n","\n","    def load_student_checkpoint(self, ckpt_path):\n","        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n","        self.student.load_state_dict(state_dict)\n","\n","    @torch.no_grad()\n","    def evaluate_student(self):\n","        self.student.cuda().eval()\n","        n = 0\n","        correct = 0\n","        for image, target in self.test_dataloader:\n","            image = image.cuda()\n","            target = target.cuda()\n","            output = self.student(image)\n","            n += image.size(0)\n","            correct += output.max(-1).indices.eq(target).sum().item()\n","        accuracy = 100 * correct / n\n","        return accuracy\n","\n","    def train_student(self):\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"6_kBr92qYURi"},"source":["# Training algorithm implementation"]},{"cell_type":"markdown","metadata":{"id":"Whrm65g0OQ0V"},"source":["In this section, you need to implement training of the student model.\n","Specifically, you need to implement the followings:\n","\n","1. Knowledge Distillation algorithm\n","2. Training pipeline\n","\n","You are free to edit below skeleton code. You are not allowed to edit the model architecture and data augmentation method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5CJM2fOOQ0W"},"outputs":[],"source":["### IMPLEMENT THIS TRAINER CLASS ###\n","\n","from torch.utils.data import Subset\n","import matplotlib.pyplot as plt\n","\n","class KDTrainer(BaseTrainer):\n","    def __init__(self, temp=43.25, lamb=38.21, wd=1e-4, lr=0.001):\n","        super().__init__()\n","        ### YOU MAY EDIT BELOW ###\n","\n","        num_train = 49984\n","        self.train_dataloader = DataLoader(self.train_set, batch_size=64, shuffle=True, num_workers=2, drop_last=True) # 49984/64=781\n","        self.mid_epoch = 180\n","        self.max_epoch = 200\n","        self.optimizer = optim.Adam(self.student.parameters(), lr=lr, betas=(0.9,0.999), weight_decay = wd)\n","        self.ce = nn.CrossEntropyLoss()\n","        self.kl = nn.KLDivLoss(reduction='batchmean')\n","        self.temp = temp\n","        self.lamb = lamb\n","        self.wd = wd\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","        ### The loss will be KL(p_s,p_ground) + \\lambda KL(q_s,q_t) * T^2\n","\n","    def train_student(self, start_epoch=0, end_epoch=None, print_result = True):\n","        #### IMPLEMENT TRAINING HERE ####\n","        loss_hist = []\n","        train_acc_hist = []\n","        test_acc_hist = []\n","\n","        self.teacher.cuda().eval()\n","        self.student.cuda().train()\n","        tdl = self.train_dataloader\n","\n","        if end_epoch == None:\n","            end_epoch = self.max_epoch\n","        for epoch in range(start_epoch, self.mid_epoch):\n","            self.student.cuda().train()\n","            correct = 0\n","            total = 0\n","            epoch_mean_loss = []\n","            if print_result:\n","                print(f\"Epoch {epoch}\")\n","            for batch_idx, (train_data, train_label) in enumerate(tdl):\n","                train_data, train_label = train_data.to(self.device), train_label.to(self.device)# on GPU\n","\n","                self.optimizer.zero_grad()\n","                soft_teacher_output = self.teacher(train_data)/self.temp\n","                student_output = self.student(train_data)\n","                soft_student_output = student_output/self.temp\n","\n","                pred = torch.max(student_output, dim=1)\n","\n","                loss = self.lamb * self.kl(F.log_softmax(soft_student_output,dim=1), F.softmax(soft_teacher_output,dim=1)) * (self.temp**2)\n","                loss += self.ce(student_output, train_label)\n","                epoch_mean_loss.append(loss.item())\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                correct += student_output.max(-1).indices.eq(train_label).sum().item()\n","                total += train_label.size(0)\n","                if print_result:\n","                    if (batch_idx+1)%200==0 or batch_idx == 780:\n","                        print(f\"Acc: {correct}/{total} \" + f\"Loss: {loss.item()}\")\n","            train_acc = correct/total*100\n","            loss_hist.append(sum(epoch_mean_loss)/len(epoch_mean_loss))\n","            train_acc_hist.append(train_acc)\n","            if print_result:\n","                print(f\"Train accuracy: {train_acc:.4f}%\")\n","            test_acc = self.evaluate_student()\n","            test_acc_hist.append(test_acc)\n","            if print_result:\n","                print(f\"Test accuracy: {test_acc:.4f}%\")\n","\n","        lr = 1e-4\n","        self.optimizer = optim.Adam(self.student.parameters(), lr=lr, betas=(0.9,0.999), weight_decay = self.wd)\n","        for epoch in range(self.mid_epoch,self.max_epoch):\n","            self.student.cuda().train()\n","            correct = 0\n","            total = 0\n","            epoch_mean_loss = []\n","            if print_result:\n","                print(f\"Epoch {epoch}\")\n","            for batch_idx, (train_data, train_label) in enumerate(tdl):\n","                train_data, train_label = train_data.to(self.device), train_label.to(self.device)# on GPU\n","\n","                self.optimizer.zero_grad()\n","                soft_teacher_output = self.teacher(train_data)/self.temp\n","                student_output = self.student(train_data)\n","                soft_student_output = student_output/self.temp\n","\n","                pred = torch.max(student_output, dim=1)\n","\n","                loss = self.lamb * self.kl(F.log_softmax(soft_student_output,dim=1), F.softmax(soft_teacher_output,dim=1)) * (self.temp**2)\n","                loss += self.ce(student_output, train_label)\n","                epoch_mean_loss.append(loss.item())\n","                loss.backward()\n","                self.optimizer.step()\n","                correct += student_output.max(-1).indices.eq(train_label).sum().item()\n","                total += train_label.size(0)\n","                if print_result:\n","                    if (batch_idx+1)%200==0 or batch_idx == 780:\n","                        print(f\"Acc: {correct}/{total} \" + f\"Loss: {loss.item()}\")\n","            train_acc = correct/total*100\n","            loss_hist.append(sum(epoch_mean_loss)/len(epoch_mean_loss))\n","            train_acc_hist.append(train_acc)\n","            if print_result:\n","                print(f\"Train accuracy: {train_acc:.4f}%\")\n","            test_acc = self.evaluate_student()\n","            test_acc_hist.append(test_acc)\n","            if print_result:\n","                print(f\"Test accuracy: {test_acc:.4f}%\")\n","        #### Plot\n","        plt.clf()\n","        epoches = list(range(1,self.max_epoch+1))\n","        plt.plot(epoches,train_acc_hist)\n","        plt.plot(epoches,test_acc_hist)\n","        plt.xlabel('epoch')\n","        plt.ylabel('Accuracy')\n","        plt.legend(('train_acc','test_acc'))\n","        plt.savefig('./acc.png', dpi=300)\n","\n","        plt.clf()\n","        plt.plot(epoches,loss_hist)\n","        plt.xlabel('epoch')\n","        plt.ylabel('Loss')\n","        plt.savefig('./loss.png', dpi=300)\n","\n","    def save_ckpt(self,epoch):\n","        state_dict = self.student.state_dict()\n","        opt_dict = self.optimizer.state_dict()\n","        torch.save({'model_state_dict':state_dict,\n","                    'opt_state_dict':opt_dict}, self.PATHPATH + f\"Epoch{epoch}.ckpt\")\n","    def load_ckpt(self,epoch):\n","        ckpt = torch.load(self.PATHPATH+f\"Epoch{epoch}.ckpt\")\n","        self.student.load_state_dict(ckpt['model_state_dict'])\n","        # self.student.cuda().eval()\n","        self.optimizer.load_state_dict(ckpt['opt_state_dict'])\n","        for state in self.optimizer.state.values():\n","            for k,v in state.items():\n","                if torch.is_tensor(v):\n","                    state[k]=v.to(self.device)\n","\n","        # weight_decay\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCL6_LQfYDPi"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import random\n","import math\n","import os\n","\n","def sample_hyp_params():\n","  temp = 10**random.uniform(np.log10(20),np.log10(80))\n","  lamb = (10**random.uniform(np.log10(10),np.log10(80)))\n","  temp = round(temp,2)\n","  lamb = round(lamb,2)\n","  # temp = 43.25\n","  # lamb = 38.21\n","  print(\"Temp: \",temp, \" Lambda: \",lamb)\n","  return temp, lamb\n","\n","def train_student(temp,lamb,lr,wd,start_epoch=0,end_epoch=100):\n","    PATHPATH = PATH+f\"T{temp:.2f}L{lamb:.2f}W{wd:.4f}\"\n","    if start_epoch == 0:\n","        os.mkdir(PATHPATH)\n","    kd = KDTrainer(temp=temp,lamb=lamb,wd=wd,lr=lr)\n","    train_acc_hist, test_acc_hist = kd.train_student(start_epoch=start_epoch, end_epoch=end_epoch, print_result=True)\n","    return train_acc_hist, test_acc_hist\n","\n","def hyp_param_search(N,wd=1e-4,lr=0.001,end_epoch=30):\n","  for i in range(N):\n","    # res = torch.load(PATH+'result.pt')\n","    temp, lamb = sample_hyp_params()\n","    PATHPATH = PATH+f\"T{temp:.2f}L{lamb:.2f}W{wd:.4f}\"\n","    while os.path.isdir(PATHPATH):\n","      temp, lamb = sample_hyp_params()\n","      PATHPATH = PATH+f\"T{temp:.2f}L{lamb:.2f}W{wd:.4f}\"\n","\n","    train_acc_hist, test_acc_hist = train_student(temp=temp,lamb=lamb,lr=lr,wd=wd,start_epoch=0,end_epoch=end_epoch)\n","\n","    print(f\"Train:{train_acc_hist[-1]}, Test:{test_acc_hist[-1]}\")\n","    # res.append([temp,lamb,train_acc_hist[-1],test_acc_hist[-1]])\n","    # torch.save(res, PATH+'result.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7BoDbRKio2P"},"outputs":[],"source":["# hyp_param_search(30,lr=1e-3)\n","# train_student(temp=43.25,lamb=38.21,lr=1e-4,wd=1e-4,start_epoch=180,end_epoch=400)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Bbb_aMjzyWN"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# from matplotlib  import cm\n","# import matplotlib as mpl\n","\n","# res = torch.load(PATH+\"result.pt\")\n","\n","# fig = plt.figure(figsize=(6,6))\n","\n","# ax = fig.add_subplot(111)\n","# ax.set_title(\"Test Accuracy\",fontsize=14)\n","# ax.set_xlabel(\"T\",fontsize=12)\n","# ax.set_xscale('log')\n","# ax.set_ylabel(\"$\\lambda$\",fontsize=12)\n","# ax.set_yscale('log')\n","\n","# ax.grid(True,linestyle='-',color='0.75')\n","# x = [re[0] for re in res]\n","# y = [re[1] for re in res]\n","# z = [re[3] for re in res]\n","# # scatter with colormap mapping to z value\n","# norm = mpl.colors.Normalize(vmin=60, vmax=70)\n","# ax.scatter(x,y,s=20,c=z, marker = 'o', cmap = cm.jet, norm=norm)\n","# mapping = cm.ScalarMappable(norm=norm,cmap=cm.jet)\n","# fig.colorbar(mappable=mapping)\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChcR2laIADKb"},"outputs":[],"source":["# res = torch.load(PATH+\"result.pt\")\n","# maxT=0\n","# maxL=0\n","# maxAcc = 0\n","# for re in res:\n","#   if re[3] >maxAcc:\n","#     maxT=re[0]\n","#     maxL=re[1]\n","#     maxAcc=re[3]\n","\n","# print(maxT,maxL,maxAcc)"]},{"cell_type":"markdown","metadata":{"id":"fMKPvEfFOQ0X"},"source":["# Training and Evaluation\n","\n","You do not need to modify the code in this section except the checkpoint path (**CKPT_PATH**).\n","\n","TAs will reproduce the results of your report with submitted checkpoint.\n","\n","**Before submission, make sure to check that the following code can print evaluation on your student model.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"c2ipeuStOQ0X","outputId":"223535bf-c33e-4041-cb28-cb997bd44d1c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1Gh3Z8BZ62PGD7PQiFiwmU9vMwMpF5F46\n","To: /content/drive/MyDrive/Advanced CNN/For submission/resnet_32x4.pth\n","100%|██████████| 59.6M/59.6M [00:00<00:00, 226MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 169001437/169001437 [00:18<00:00, 9210396.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-100-python.tar.gz to ./data/\n","Epoch 0\n","Acc: 1014/12800 Loss: 83.90213775634766\n","Acc: 2928/25600 Loss: 78.42284393310547\n","Acc: 5444/38400 Loss: 80.26675415039062\n","Acc: 8087/49984 Loss: 75.66827392578125\n","Train accuracy: 16.1792%\n","Test accuracy: 22.8400%\n","Epoch 1\n","Acc: 3332/12800 Loss: 76.1744155883789\n","Acc: 6942/25600 Loss: 64.49232482910156\n","Acc: 10843/38400 Loss: 73.93506622314453\n","Acc: 14722/49984 Loss: 71.06636810302734\n","Train accuracy: 29.4534%\n","Test accuracy: 32.6700%\n","Epoch 2\n","Acc: 4567/12800 Loss: 66.38475799560547\n","Acc: 9329/25600 Loss: 61.21508026123047\n","Acc: 14247/38400 Loss: 63.22962951660156\n","Acc: 18832/49984 Loss: 63.32634735107422\n","Train accuracy: 37.6761%\n","Test accuracy: 38.0000%\n","Epoch 3\n","Acc: 5425/12800 Loss: 60.434181213378906\n","Acc: 11027/25600 Loss: 66.92382049560547\n","Acc: 16683/38400 Loss: 57.52893829345703\n","Acc: 21929/49984 Loss: 61.339256286621094\n","Train accuracy: 43.8720%\n","Test accuracy: 40.8200%\n","Epoch 4\n","Acc: 6042/12800 Loss: 57.964996337890625\n","Acc: 12153/25600 Loss: 56.10627365112305\n","Acc: 18354/38400 Loss: 57.060672760009766\n","Acc: 24089/49984 Loss: 51.7115592956543\n","Train accuracy: 48.1934%\n","Test accuracy: 47.9200%\n","Epoch 5\n","Acc: 6602/12800 Loss: 60.465023040771484\n","Acc: 13159/25600 Loss: 56.28047561645508\n","Acc: 19791/38400 Loss: 51.16817092895508\n","Acc: 25921/49984 Loss: 54.36785888671875\n","Train accuracy: 51.8586%\n","Test accuracy: 49.2800%\n","Epoch 6\n","Acc: 6987/12800 Loss: 59.30747604370117\n","Acc: 14005/25600 Loss: 52.97815704345703\n","Acc: 20974/38400 Loss: 53.0645637512207\n","Acc: 27438/49984 Loss: 60.96013641357422\n","Train accuracy: 54.8936%\n","Test accuracy: 52.8200%\n","Epoch 7\n","Acc: 7297/12800 Loss: 47.063785552978516\n","Acc: 14629/25600 Loss: 51.599700927734375\n","Acc: 22027/38400 Loss: 47.91487121582031\n","Acc: 28747/49984 Loss: 47.72795867919922\n","Train accuracy: 57.5124%\n","Test accuracy: 54.6800%\n","Epoch 8\n","Acc: 7638/12800 Loss: 52.79497146606445\n","Acc: 15334/25600 Loss: 47.25807571411133\n","Acc: 22963/38400 Loss: 43.80744934082031\n","Acc: 29949/49984 Loss: 49.40693283081055\n","Train accuracy: 59.9172%\n","Test accuracy: 56.6800%\n","Epoch 9\n","Acc: 7934/12800 Loss: 48.36728286743164\n","Acc: 15729/25600 Loss: 53.997047424316406\n","Acc: 23562/38400 Loss: 53.58412170410156\n","Acc: 30746/49984 Loss: 42.874149322509766\n","Train accuracy: 61.5117%\n","Test accuracy: 57.0500%\n","Epoch 10\n","Acc: 8018/12800 Loss: 47.95301818847656\n","Acc: 16079/25600 Loss: 49.73418045043945\n","Acc: 24126/38400 Loss: 44.75746536254883\n","Acc: 31439/49984 Loss: 39.77256774902344\n","Train accuracy: 62.8981%\n","Test accuracy: 59.8900%\n","Epoch 11\n","Acc: 8288/12800 Loss: 45.53976821899414\n","Acc: 16532/25600 Loss: 47.90788650512695\n","Acc: 24775/38400 Loss: 40.98259735107422\n","Acc: 32201/49984 Loss: 43.712467193603516\n","Train accuracy: 64.4226%\n","Test accuracy: 60.7400%\n","Epoch 12\n","Acc: 8427/12800 Loss: 46.03745651245117\n","Acc: 16919/25600 Loss: 44.49271774291992\n","Acc: 25343/38400 Loss: 41.36732864379883\n","Acc: 32843/49984 Loss: 44.96870422363281\n","Train accuracy: 65.7070%\n","Test accuracy: 61.0600%\n","Epoch 13\n","Acc: 8623/12800 Loss: 42.40399932861328\n","Acc: 17231/25600 Loss: 44.12771987915039\n","Acc: 25733/38400 Loss: 42.50148391723633\n","Acc: 33342/49984 Loss: 45.36856460571289\n","Train accuracy: 66.7053%\n","Test accuracy: 60.6200%\n","Epoch 14\n","Acc: 8739/12800 Loss: 32.48343276977539\n","Acc: 17443/25600 Loss: 38.984375\n","Acc: 26001/38400 Loss: 49.71575164794922\n","Acc: 33836/49984 Loss: 43.583248138427734\n","Train accuracy: 67.6937%\n","Test accuracy: 62.9500%\n","Epoch 15\n","Acc: 8757/12800 Loss: 36.19013595581055\n","Acc: 17523/25600 Loss: 39.43638229370117\n","Acc: 26383/38400 Loss: 40.866580963134766\n","Acc: 34269/49984 Loss: 37.611392974853516\n","Train accuracy: 68.5599%\n","Test accuracy: 62.6400%\n","Epoch 16\n","Acc: 8924/12800 Loss: 34.10708999633789\n","Acc: 17776/25600 Loss: 42.793697357177734\n","Acc: 26713/38400 Loss: 39.5247917175293\n","Acc: 34741/49984 Loss: 37.54579544067383\n","Train accuracy: 69.5042%\n","Test accuracy: 63.8900%\n","Epoch 17\n","Acc: 8972/12800 Loss: 40.11178207397461\n","Acc: 18022/25600 Loss: 39.555110931396484\n","Acc: 26923/38400 Loss: 38.60629653930664\n","Acc: 35110/49984 Loss: 34.193939208984375\n","Train accuracy: 70.2425%\n","Test accuracy: 63.6600%\n","Epoch 18\n","Acc: 9167/12800 Loss: 41.353912353515625\n","Acc: 18160/25600 Loss: 32.100341796875\n","Acc: 27196/38400 Loss: 39.894798278808594\n","Acc: 35367/49984 Loss: 37.236473083496094\n","Train accuracy: 70.7566%\n","Test accuracy: 64.9500%\n","Epoch 19\n","Acc: 9123/12800 Loss: 42.024864196777344\n","Acc: 18224/25600 Loss: 38.400970458984375\n","Acc: 27340/38400 Loss: 40.811275482177734\n","Acc: 35606/49984 Loss: 40.2839241027832\n","Train accuracy: 71.2348%\n","Test accuracy: 65.3800%\n","Epoch 20\n","Acc: 9325/12800 Loss: 38.67308807373047\n","Acc: 18612/25600 Loss: 36.022003173828125\n","Acc: 27734/38400 Loss: 38.31312561035156\n","Acc: 36002/49984 Loss: 35.37515640258789\n","Train accuracy: 72.0270%\n","Test accuracy: 65.8400%\n","Epoch 21\n","Acc: 9314/12800 Loss: 37.36456298828125\n","Acc: 18657/25600 Loss: 41.4967041015625\n","Acc: 27951/38400 Loss: 37.0434684753418\n","Acc: 36239/49984 Loss: 40.01571273803711\n","Train accuracy: 72.5012%\n","Test accuracy: 65.7800%\n","Epoch 22\n","Acc: 9387/12800 Loss: 40.06028366088867\n","Acc: 18721/25600 Loss: 36.062374114990234\n","Acc: 28085/38400 Loss: 39.329246520996094\n","Acc: 36453/49984 Loss: 36.37934875488281\n","Train accuracy: 72.9293%\n","Test accuracy: 65.4800%\n","Epoch 23\n","Acc: 9480/12800 Loss: 33.356422424316406\n","Acc: 19017/25600 Loss: 36.459529876708984\n","Acc: 28434/38400 Loss: 39.39094543457031\n","Acc: 36813/49984 Loss: 33.91952133178711\n","Train accuracy: 73.6496%\n","Test accuracy: 65.2700%\n","Epoch 24\n","Acc: 9579/12800 Loss: 44.34614181518555\n","Acc: 19064/25600 Loss: 36.315834045410156\n","Acc: 28558/38400 Loss: 31.15988540649414\n","Acc: 37082/49984 Loss: 34.79997253417969\n","Train accuracy: 74.1877%\n","Test accuracy: 65.9200%\n","Epoch 25\n","Acc: 9621/12800 Loss: 35.29000473022461\n","Acc: 19099/25600 Loss: 39.92320251464844\n","Acc: 28633/38400 Loss: 32.535648345947266\n","Acc: 37238/49984 Loss: 36.308555603027344\n","Train accuracy: 74.4998%\n","Test accuracy: 65.2500%\n","Epoch 26\n","Acc: 9733/12800 Loss: 34.80329513549805\n","Acc: 19322/25600 Loss: 32.839168548583984\n","Acc: 28831/38400 Loss: 36.80302047729492\n","Acc: 37476/49984 Loss: 36.06996154785156\n","Train accuracy: 74.9760%\n","Test accuracy: 65.8200%\n","Epoch 27\n","Acc: 9750/12800 Loss: 31.10390853881836\n","Acc: 19354/25600 Loss: 27.631452560424805\n","Acc: 29029/38400 Loss: 35.4943962097168\n","Acc: 37628/49984 Loss: 29.726564407348633\n","Train accuracy: 75.2801%\n","Test accuracy: 67.2100%\n","Epoch 28\n","Acc: 9776/12800 Loss: 35.65703582763672\n","Acc: 19426/25600 Loss: 33.77424240112305\n","Acc: 29036/38400 Loss: 33.641483306884766\n","Acc: 37768/49984 Loss: 34.07651901245117\n","Train accuracy: 75.5602%\n","Test accuracy: 66.7800%\n","Epoch 29\n","Acc: 9793/12800 Loss: 28.175085067749023\n","Acc: 19543/25600 Loss: 36.40336990356445\n","Acc: 29248/38400 Loss: 30.092092514038086\n","Acc: 38001/49984 Loss: 34.736026763916016\n","Train accuracy: 76.0263%\n","Test accuracy: 66.5800%\n","Epoch 30\n","Acc: 9879/12800 Loss: 38.7627067565918\n","Acc: 19646/25600 Loss: 28.99529457092285\n","Acc: 29445/38400 Loss: 31.20714569091797\n","Acc: 38284/49984 Loss: 32.28593826293945\n","Train accuracy: 76.5925%\n","Test accuracy: 67.3600%\n","Epoch 31\n","Acc: 9928/12800 Loss: 34.53936004638672\n","Acc: 19762/25600 Loss: 30.934215545654297\n","Acc: 29544/38400 Loss: 34.340030670166016\n","Acc: 38397/49984 Loss: 37.67795181274414\n","Train accuracy: 76.8186%\n","Test accuracy: 67.4200%\n","Epoch 32\n","Acc: 9878/12800 Loss: 34.34123611450195\n","Acc: 19760/25600 Loss: 30.089658737182617\n","Acc: 29589/38400 Loss: 39.783348083496094\n","Acc: 38454/49984 Loss: 31.472244262695312\n","Train accuracy: 76.9326%\n","Test accuracy: 67.5100%\n","Epoch 33\n","Acc: 9947/12800 Loss: 30.592578887939453\n","Acc: 19911/25600 Loss: 34.866050720214844\n","Acc: 29801/38400 Loss: 32.9925422668457\n","Acc: 38672/49984 Loss: 32.514404296875\n","Train accuracy: 77.3688%\n","Test accuracy: 68.1300%\n","Epoch 34\n","Acc: 10047/12800 Loss: 33.1138916015625\n","Acc: 19962/25600 Loss: 34.62343215942383\n","Acc: 29906/38400 Loss: 32.01778030395508\n","Acc: 38823/49984 Loss: 30.214448928833008\n","Train accuracy: 77.6709%\n","Test accuracy: 68.1400%\n","Epoch 35\n","Acc: 10061/12800 Loss: 31.505319595336914\n","Acc: 20061/25600 Loss: 31.67416763305664\n","Acc: 30018/38400 Loss: 35.943214416503906\n","Acc: 38982/49984 Loss: 31.480337142944336\n","Train accuracy: 77.9890%\n","Test accuracy: 68.6200%\n","Epoch 36\n","Acc: 10151/12800 Loss: 34.982967376708984\n","Acc: 20251/25600 Loss: 31.097518920898438\n","Acc: 30146/38400 Loss: 29.98236656188965\n","Acc: 39186/49984 Loss: 29.076480865478516\n","Train accuracy: 78.3971%\n","Test accuracy: 68.1600%\n","Epoch 37\n","Acc: 10120/12800 Loss: 37.61425018310547\n","Acc: 20189/25600 Loss: 30.66468620300293\n","Acc: 30214/38400 Loss: 36.31247329711914\n","Acc: 39261/49984 Loss: 28.08719825744629\n","Train accuracy: 78.5471%\n","Test accuracy: 68.3000%\n","Epoch 38\n","Acc: 10128/12800 Loss: 31.30268096923828\n","Acc: 20307/25600 Loss: 28.865400314331055\n","Acc: 30399/38400 Loss: 33.46255111694336\n","Acc: 39404/49984 Loss: 31.215436935424805\n","Train accuracy: 78.8332%\n","Test accuracy: 68.3900%\n","Epoch 39\n","Acc: 10209/12800 Loss: 31.83027458190918\n","Acc: 20360/25600 Loss: 30.606578826904297\n","Acc: 30446/38400 Loss: 27.74298858642578\n","Acc: 39532/49984 Loss: 30.531204223632812\n","Train accuracy: 79.0893%\n","Test accuracy: 68.8900%\n","Epoch 40\n","Acc: 10199/12800 Loss: 33.22996139526367\n","Acc: 20352/25600 Loss: 35.45821762084961\n","Acc: 30503/38400 Loss: 30.06783676147461\n","Acc: 39630/49984 Loss: 27.602352142333984\n","Train accuracy: 79.2854%\n","Test accuracy: 69.0000%\n","Epoch 41\n","Acc: 10345/12800 Loss: 33.50663375854492\n","Acc: 20519/25600 Loss: 29.11558723449707\n","Acc: 30644/38400 Loss: 25.370162963867188\n","Acc: 39784/49984 Loss: 27.120969772338867\n","Train accuracy: 79.5935%\n","Test accuracy: 68.5300%\n","Epoch 42\n","Acc: 10341/12800 Loss: 32.40144729614258\n","Acc: 20539/25600 Loss: 31.605684280395508\n","Acc: 30696/38400 Loss: 31.98624610900879\n","Acc: 39916/49984 Loss: 33.07342529296875\n","Train accuracy: 79.8576%\n","Test accuracy: 69.0800%\n","Epoch 43\n","Acc: 10305/12800 Loss: 27.21756362915039\n","Acc: 20543/25600 Loss: 34.566036224365234\n","Acc: 30737/38400 Loss: 32.90865707397461\n","Acc: 39900/49984 Loss: 32.006996154785156\n","Train accuracy: 79.8255%\n","Test accuracy: 69.1500%\n","Epoch 44\n","Acc: 10362/12800 Loss: 33.265506744384766\n","Acc: 20564/25600 Loss: 27.528732299804688\n","Acc: 30755/38400 Loss: 31.34285545349121\n","Acc: 39990/49984 Loss: 31.52408790588379\n","Train accuracy: 80.0056%\n","Test accuracy: 68.8300%\n","Epoch 45\n","Acc: 10399/12800 Loss: 30.432119369506836\n","Acc: 20679/25600 Loss: 27.39580535888672\n","Acc: 30926/38400 Loss: 28.55907440185547\n","Acc: 40227/49984 Loss: 30.097196578979492\n","Train accuracy: 80.4798%\n","Test accuracy: 68.7800%\n","Epoch 46\n","Acc: 10403/12800 Loss: 31.59427261352539\n","Acc: 20781/25600 Loss: 30.215206146240234\n","Acc: 31018/38400 Loss: 28.850757598876953\n","Acc: 40348/49984 Loss: 27.16562271118164\n","Train accuracy: 80.7218%\n","Test accuracy: 68.8400%\n","Epoch 47\n","Acc: 10389/12800 Loss: 30.582042694091797\n","Acc: 20803/25600 Loss: 28.21760368347168\n","Acc: 31117/38400 Loss: 29.174476623535156\n","Acc: 40441/49984 Loss: 29.178173065185547\n","Train accuracy: 80.9079%\n","Test accuracy: 69.6100%\n","Epoch 48\n","Acc: 10488/12800 Loss: 29.148454666137695\n","Acc: 20868/25600 Loss: 33.24953842163086\n","Acc: 31247/38400 Loss: 28.455318450927734\n","Acc: 40516/49984 Loss: 28.17615509033203\n","Train accuracy: 81.0579%\n","Test accuracy: 69.7500%\n","Epoch 49\n","Acc: 10579/12800 Loss: 27.509397506713867\n","Acc: 20917/25600 Loss: 29.37714385986328\n","Acc: 31275/38400 Loss: 26.464128494262695\n","Acc: 40652/49984 Loss: 35.702552795410156\n","Train accuracy: 81.3300%\n","Test accuracy: 70.1000%\n","Epoch 50\n","Acc: 10512/12800 Loss: 32.39215087890625\n","Acc: 20829/25600 Loss: 28.06417465209961\n","Acc: 31291/38400 Loss: 33.147979736328125\n","Acc: 40665/49984 Loss: 32.423648834228516\n","Train accuracy: 81.3560%\n","Test accuracy: 69.4200%\n","Epoch 51\n","Acc: 10535/12800 Loss: 27.612897872924805\n","Acc: 20987/25600 Loss: 29.9874324798584\n","Acc: 31354/38400 Loss: 30.446250915527344\n","Acc: 40779/49984 Loss: 30.34152603149414\n","Train accuracy: 81.5841%\n","Test accuracy: 69.1100%\n","Epoch 52\n","Acc: 10578/12800 Loss: 31.2253360748291\n","Acc: 21007/25600 Loss: 28.827198028564453\n","Acc: 31490/38400 Loss: 32.680206298828125\n","Acc: 40889/49984 Loss: 26.823375701904297\n","Train accuracy: 81.8042%\n","Test accuracy: 69.5300%\n","Epoch 53\n","Acc: 10614/12800 Loss: 26.709115982055664\n","Acc: 21167/25600 Loss: 32.43782043457031\n","Acc: 31558/38400 Loss: 31.542293548583984\n","Acc: 41004/49984 Loss: 26.983436584472656\n","Train accuracy: 82.0343%\n","Test accuracy: 69.4300%\n","Epoch 54\n","Acc: 10604/12800 Loss: 27.74976348876953\n","Acc: 21059/25600 Loss: 26.01546859741211\n","Acc: 31576/38400 Loss: 31.859357833862305\n","Acc: 41060/49984 Loss: 30.50524139404297\n","Train accuracy: 82.1463%\n","Test accuracy: 68.9400%\n","Epoch 55\n","Acc: 10603/12800 Loss: 28.046932220458984\n","Acc: 21132/25600 Loss: 29.27115821838379\n","Acc: 31648/38400 Loss: 25.37211036682129\n","Acc: 41091/49984 Loss: 32.869300842285156\n","Train accuracy: 82.2083%\n","Test accuracy: 69.7200%\n","Epoch 56\n","Acc: 10637/12800 Loss: 29.18345069885254\n","Acc: 21258/25600 Loss: 32.543575286865234\n","Acc: 31788/38400 Loss: 27.701473236083984\n","Acc: 41208/49984 Loss: 31.84238052368164\n","Train accuracy: 82.4424%\n","Test accuracy: 69.7900%\n","Epoch 57\n","Acc: 10755/12800 Loss: 32.400909423828125\n","Acc: 21307/25600 Loss: 26.486083984375\n","Acc: 31797/38400 Loss: 25.456178665161133\n","Acc: 41267/49984 Loss: 33.68838882446289\n","Train accuracy: 82.5604%\n","Test accuracy: 69.8200%\n","Epoch 58\n","Acc: 10627/12800 Loss: 27.291427612304688\n","Acc: 21228/25600 Loss: 27.111244201660156\n","Acc: 31799/38400 Loss: 25.274581909179688\n","Acc: 41330/49984 Loss: 30.840234756469727\n","Train accuracy: 82.6865%\n","Test accuracy: 70.2700%\n","Epoch 59\n","Acc: 10675/12800 Loss: 31.736024856567383\n","Acc: 21327/25600 Loss: 24.841110229492188\n","Acc: 31882/38400 Loss: 28.693950653076172\n","Acc: 41456/49984 Loss: 25.19646453857422\n","Train accuracy: 82.9385%\n","Test accuracy: 70.0100%\n","Epoch 60\n","Acc: 10707/12800 Loss: 28.343809127807617\n","Acc: 21391/25600 Loss: 24.068239212036133\n","Acc: 31982/38400 Loss: 23.807729721069336\n","Acc: 41473/49984 Loss: 28.633275985717773\n","Train accuracy: 82.9726%\n","Test accuracy: 69.2400%\n","Epoch 61\n","Acc: 10726/12800 Loss: 28.125202178955078\n","Acc: 21332/25600 Loss: 28.960803985595703\n","Acc: 32018/38400 Loss: 25.392166137695312\n","Acc: 41566/49984 Loss: 25.423852920532227\n","Train accuracy: 83.1586%\n","Test accuracy: 69.6400%\n","Epoch 62\n","Acc: 10657/12800 Loss: 26.596519470214844\n","Acc: 21363/25600 Loss: 33.372703552246094\n","Acc: 32006/38400 Loss: 30.697336196899414\n","Acc: 41594/49984 Loss: 25.651105880737305\n","Train accuracy: 83.2146%\n","Test accuracy: 70.3700%\n","Epoch 63\n","Acc: 10739/12800 Loss: 27.828922271728516\n","Acc: 21442/25600 Loss: 27.141958236694336\n","Acc: 32087/38400 Loss: 29.65316390991211\n","Acc: 41680/49984 Loss: 28.684553146362305\n","Train accuracy: 83.3867%\n","Test accuracy: 70.8600%\n","Epoch 64\n","Acc: 10773/12800 Loss: 26.065210342407227\n","Acc: 21434/25600 Loss: 27.962373733520508\n","Acc: 32134/38400 Loss: 27.269359588623047\n","Acc: 41776/49984 Loss: 32.41025161743164\n","Train accuracy: 83.5787%\n","Test accuracy: 70.2300%\n","Epoch 65\n","Acc: 10766/12800 Loss: 28.787384033203125\n","Acc: 21416/25600 Loss: 30.628589630126953\n","Acc: 32080/38400 Loss: 29.520450592041016\n","Acc: 41744/49984 Loss: 30.88593292236328\n","Train accuracy: 83.5147%\n","Test accuracy: 70.0200%\n","Epoch 66\n","Acc: 10825/12800 Loss: 28.249361038208008\n","Acc: 21608/25600 Loss: 28.213342666625977\n","Acc: 32399/38400 Loss: 26.20090675354004\n","Acc: 42062/49984 Loss: 26.716520309448242\n","Train accuracy: 84.1509%\n","Test accuracy: 70.4000%\n","Epoch 67\n","Acc: 10844/12800 Loss: 25.302885055541992\n","Acc: 21630/25600 Loss: 23.564409255981445\n","Acc: 32346/38400 Loss: 22.958959579467773\n","Acc: 41953/49984 Loss: 26.69243812561035\n","Train accuracy: 83.9329%\n","Test accuracy: 70.6700%\n","Epoch 68\n","Acc: 10821/12800 Loss: 29.36276626586914\n","Acc: 21648/25600 Loss: 28.05282211303711\n","Acc: 32372/38400 Loss: 25.800012588500977\n","Acc: 42072/49984 Loss: 29.554338455200195\n","Train accuracy: 84.1709%\n","Test accuracy: 70.1300%\n","Epoch 69\n","Acc: 10840/12800 Loss: 24.431575775146484\n","Acc: 21676/25600 Loss: 25.647201538085938\n","Acc: 32389/38400 Loss: 28.58419418334961\n","Acc: 42051/49984 Loss: 26.261932373046875\n","Train accuracy: 84.1289%\n","Test accuracy: 70.2600%\n","Epoch 70\n","Acc: 10914/12800 Loss: 27.53579330444336\n","Acc: 21717/25600 Loss: 29.08687400817871\n","Acc: 32488/38400 Loss: 29.264291763305664\n","Acc: 42167/49984 Loss: 27.780458450317383\n","Train accuracy: 84.3610%\n","Test accuracy: 70.2900%\n","Epoch 71\n","Acc: 10856/12800 Loss: 27.41021156311035\n","Acc: 21651/25600 Loss: 25.412023544311523\n","Acc: 32498/38400 Loss: 27.291141510009766\n","Acc: 42247/49984 Loss: 28.03653907775879\n","Train accuracy: 84.5210%\n","Test accuracy: 70.1900%\n","Epoch 72\n","Acc: 10881/12800 Loss: 25.462947845458984\n","Acc: 21731/25600 Loss: 28.900434494018555\n","Acc: 32552/38400 Loss: 24.657127380371094\n","Acc: 42306/49984 Loss: 25.527820587158203\n","Train accuracy: 84.6391%\n","Test accuracy: 69.6700%\n","Epoch 73\n","Acc: 10883/12800 Loss: 25.264129638671875\n","Acc: 21758/25600 Loss: 26.57039451599121\n","Acc: 32628/38400 Loss: 28.823137283325195\n","Acc: 42376/49984 Loss: 23.675731658935547\n","Train accuracy: 84.7791%\n","Test accuracy: 70.7600%\n","Epoch 74\n","Acc: 10952/12800 Loss: 30.86281394958496\n","Acc: 21801/25600 Loss: 22.8537654876709\n","Acc: 32636/38400 Loss: 26.697603225708008\n","Acc: 42413/49984 Loss: 21.349056243896484\n","Train accuracy: 84.8532%\n","Test accuracy: 70.9400%\n","Epoch 75\n","Acc: 10897/12800 Loss: 29.09981918334961\n","Acc: 21769/25600 Loss: 28.994583129882812\n","Acc: 32619/38400 Loss: 24.426727294921875\n","Acc: 42401/49984 Loss: 22.105154037475586\n","Train accuracy: 84.8291%\n","Test accuracy: 70.4400%\n","Epoch 76\n","Acc: 11022/12800 Loss: 32.900699615478516\n","Acc: 21888/25600 Loss: 29.78750991821289\n","Acc: 32771/38400 Loss: 25.196002960205078\n","Acc: 42555/49984 Loss: 24.737449645996094\n","Train accuracy: 85.1372%\n","Test accuracy: 70.5100%\n","Epoch 77\n","Acc: 10956/12800 Loss: 24.67448616027832\n","Acc: 21885/25600 Loss: 23.546737670898438\n","Acc: 32724/38400 Loss: 28.885765075683594\n","Acc: 42517/49984 Loss: 25.5741024017334\n","Train accuracy: 85.0612%\n","Test accuracy: 70.8000%\n","Epoch 78\n","Acc: 11023/12800 Loss: 24.86065673828125\n","Acc: 21971/25600 Loss: 30.850088119506836\n","Acc: 32822/38400 Loss: 25.45796775817871\n","Acc: 42551/49984 Loss: 26.288070678710938\n","Train accuracy: 85.1292%\n","Test accuracy: 70.4200%\n","Epoch 79\n","Acc: 10999/12800 Loss: 23.79528045654297\n","Acc: 21969/25600 Loss: 23.0926570892334\n","Acc: 32821/38400 Loss: 24.434106826782227\n","Acc: 42610/49984 Loss: 27.031360626220703\n","Train accuracy: 85.2473%\n","Test accuracy: 71.0400%\n","Epoch 80\n","Acc: 11022/12800 Loss: 27.795120239257812\n","Acc: 22011/25600 Loss: 26.303804397583008\n","Acc: 32903/38400 Loss: 24.284025192260742\n","Acc: 42661/49984 Loss: 28.24357032775879\n","Train accuracy: 85.3493%\n","Test accuracy: 71.3700%\n","Epoch 81\n","Acc: 11027/12800 Loss: 22.97968864440918\n","Acc: 21959/25600 Loss: 26.230636596679688\n","Acc: 32896/38400 Loss: 25.49581527709961\n","Acc: 42776/49984 Loss: 29.123920440673828\n","Train accuracy: 85.5794%\n","Test accuracy: 70.5400%\n","Epoch 82\n","Acc: 11041/12800 Loss: 27.7830867767334\n","Acc: 21963/25600 Loss: 30.685522079467773\n","Acc: 32956/38400 Loss: 26.01331329345703\n","Acc: 42825/49984 Loss: 26.378267288208008\n","Train accuracy: 85.6774%\n","Test accuracy: 70.5900%\n","Epoch 83\n","Acc: 11057/12800 Loss: 20.954832077026367\n","Acc: 22079/25600 Loss: 28.267122268676758\n","Acc: 33000/38400 Loss: 26.99687957763672\n","Acc: 42922/49984 Loss: 28.164770126342773\n","Train accuracy: 85.8715%\n","Test accuracy: 70.3600%\n","Epoch 84\n","Acc: 11137/12800 Loss: 27.807451248168945\n","Acc: 22121/25600 Loss: 26.666179656982422\n","Acc: 33029/38400 Loss: 27.43142318725586\n","Acc: 42921/49984 Loss: 25.553709030151367\n","Train accuracy: 85.8695%\n","Test accuracy: 70.8000%\n","Epoch 85\n","Acc: 11045/12800 Loss: 25.284805297851562\n","Acc: 22119/25600 Loss: 28.51229476928711\n","Acc: 33138/38400 Loss: 22.570819854736328\n","Acc: 42997/49984 Loss: 24.74358367919922\n","Train accuracy: 86.0215%\n","Test accuracy: 70.8300%\n","Epoch 86\n","Acc: 11092/12800 Loss: 24.715307235717773\n","Acc: 22138/25600 Loss: 25.240510940551758\n","Acc: 33140/38400 Loss: 28.57992172241211\n","Acc: 43079/49984 Loss: 27.258167266845703\n","Train accuracy: 86.1856%\n","Test accuracy: 71.1700%\n","Epoch 87\n","Acc: 11164/12800 Loss: 22.17091941833496\n","Acc: 22258/25600 Loss: 24.456701278686523\n","Acc: 33157/38400 Loss: 23.390600204467773\n","Acc: 43052/49984 Loss: 26.779300689697266\n","Train accuracy: 86.1316%\n","Test accuracy: 70.7000%\n","Epoch 88\n","Acc: 11011/12800 Loss: 24.09299087524414\n","Acc: 22105/25600 Loss: 24.970306396484375\n","Acc: 33144/38400 Loss: 24.017623901367188\n","Acc: 43098/49984 Loss: 25.869415283203125\n","Train accuracy: 86.2236%\n","Test accuracy: 71.2200%\n","Epoch 89\n","Acc: 11171/12800 Loss: 25.351768493652344\n","Acc: 22262/25600 Loss: 24.953227996826172\n","Acc: 33282/38400 Loss: 23.694883346557617\n","Acc: 43259/49984 Loss: 27.014141082763672\n","Train accuracy: 86.5457%\n","Test accuracy: 71.2100%\n","Epoch 90\n","Acc: 11148/12800 Loss: 19.96221160888672\n","Acc: 22208/25600 Loss: 24.76068115234375\n","Acc: 33225/38400 Loss: 25.249494552612305\n","Acc: 43197/49984 Loss: 28.53652000427246\n","Train accuracy: 86.4217%\n","Test accuracy: 71.1100%\n","Epoch 91\n","Acc: 11196/12800 Loss: 23.774030685424805\n","Acc: 22318/25600 Loss: 26.497148513793945\n","Acc: 33345/38400 Loss: 26.35117530822754\n","Acc: 43305/49984 Loss: 24.046655654907227\n","Train accuracy: 86.6377%\n","Test accuracy: 71.2300%\n","Epoch 92\n","Acc: 11169/12800 Loss: 25.307538986206055\n","Acc: 22317/25600 Loss: 23.285791397094727\n","Acc: 33326/38400 Loss: 23.94347381591797\n","Acc: 43272/49984 Loss: 25.676610946655273\n","Train accuracy: 86.5717%\n","Test accuracy: 71.1800%\n","Epoch 93\n","Acc: 11155/12800 Loss: 24.090717315673828\n","Acc: 22269/25600 Loss: 23.45941925048828\n","Acc: 33345/38400 Loss: 25.434316635131836\n","Acc: 43299/49984 Loss: 25.755924224853516\n","Train accuracy: 86.6257%\n","Test accuracy: 71.3400%\n","Epoch 94\n","Acc: 11172/12800 Loss: 22.719223022460938\n","Acc: 22306/25600 Loss: 25.795501708984375\n","Acc: 33335/38400 Loss: 28.101682662963867\n","Acc: 43324/49984 Loss: 23.60068702697754\n","Train accuracy: 86.6757%\n","Test accuracy: 71.4200%\n","Epoch 95\n","Acc: 11156/12800 Loss: 22.270048141479492\n","Acc: 22266/25600 Loss: 24.247995376586914\n","Acc: 33357/38400 Loss: 26.814449310302734\n","Acc: 43383/49984 Loss: 25.10655403137207\n","Train accuracy: 86.7938%\n","Test accuracy: 71.3500%\n","Epoch 96\n","Acc: 11281/12800 Loss: 20.23105239868164\n","Acc: 22450/25600 Loss: 27.536117553710938\n","Acc: 33546/38400 Loss: 21.99848747253418\n","Acc: 43523/49984 Loss: 29.353286743164062\n","Train accuracy: 87.0739%\n","Test accuracy: 71.0500%\n","Epoch 97\n","Acc: 11249/12800 Loss: 23.913307189941406\n","Acc: 22354/25600 Loss: 24.47296142578125\n","Acc: 33433/38400 Loss: 25.119670867919922\n","Acc: 43438/49984 Loss: 27.124927520751953\n","Train accuracy: 86.9038%\n","Test accuracy: 71.5300%\n","Epoch 98\n","Acc: 11295/12800 Loss: 24.969833374023438\n","Acc: 22394/25600 Loss: 22.029804229736328\n","Acc: 33490/38400 Loss: 20.68549346923828\n","Acc: 43463/49984 Loss: 27.22686767578125\n","Train accuracy: 86.9538%\n","Test accuracy: 71.1200%\n","Epoch 99\n","Acc: 11240/12800 Loss: 21.4522762298584\n","Acc: 22494/25600 Loss: 22.195642471313477\n","Acc: 33672/38400 Loss: 28.523189544677734\n","Acc: 43688/49984 Loss: 24.156702041625977\n","Train accuracy: 87.4040%\n","Test accuracy: 71.0500%\n","Epoch 100\n","Acc: 11260/12800 Loss: 26.57413673400879\n","Acc: 22373/25600 Loss: 21.30962562561035\n","Acc: 33512/38400 Loss: 21.717182159423828\n","Acc: 43581/49984 Loss: 24.539569854736328\n","Train accuracy: 87.1899%\n","Test accuracy: 71.1400%\n","Epoch 101\n","Acc: 11256/12800 Loss: 28.5694522857666\n","Acc: 22418/25600 Loss: 20.84499168395996\n","Acc: 33581/38400 Loss: 26.839599609375\n","Acc: 43624/49984 Loss: 28.661306381225586\n","Train accuracy: 87.2759%\n","Test accuracy: 71.5400%\n","Epoch 102\n","Acc: 11203/12800 Loss: 26.068679809570312\n","Acc: 22342/25600 Loss: 26.815885543823242\n","Acc: 33586/38400 Loss: 23.686904907226562\n","Acc: 43684/49984 Loss: 25.239484786987305\n","Train accuracy: 87.3960%\n","Test accuracy: 70.9900%\n","Epoch 103\n","Acc: 11286/12800 Loss: 23.611623764038086\n","Acc: 22453/25600 Loss: 22.110374450683594\n","Acc: 33594/38400 Loss: 25.165908813476562\n","Acc: 43623/49984 Loss: 22.780487060546875\n","Train accuracy: 87.2739%\n","Test accuracy: 71.1300%\n","Epoch 104\n","Acc: 11253/12800 Loss: 21.509029388427734\n","Acc: 22460/25600 Loss: 25.572813034057617\n","Acc: 33619/38400 Loss: 20.778438568115234\n","Acc: 43664/49984 Loss: 28.98077964782715\n","Train accuracy: 87.3560%\n","Test accuracy: 70.6700%\n","Epoch 105\n","Acc: 11342/12800 Loss: 22.398296356201172\n","Acc: 22602/25600 Loss: 24.99486541748047\n","Acc: 33722/38400 Loss: 21.690269470214844\n","Acc: 43826/49984 Loss: 22.21370506286621\n","Train accuracy: 87.6801%\n","Test accuracy: 71.1000%\n","Epoch 106\n","Acc: 11322/12800 Loss: 23.448265075683594\n","Acc: 22572/25600 Loss: 20.26882553100586\n","Acc: 33677/38400 Loss: 25.213850021362305\n","Acc: 43763/49984 Loss: 22.569034576416016\n","Train accuracy: 87.5540%\n","Test accuracy: 71.2600%\n","Epoch 107\n","Acc: 11302/12800 Loss: 24.474756240844727\n","Acc: 22558/25600 Loss: 22.437911987304688\n","Acc: 33710/38400 Loss: 25.37226676940918\n","Acc: 43803/49984 Loss: 23.715717315673828\n","Train accuracy: 87.6340%\n","Test accuracy: 71.4900%\n","Epoch 108\n","Acc: 11294/12800 Loss: 22.30424690246582\n","Acc: 22616/25600 Loss: 20.81806182861328\n","Acc: 33776/38400 Loss: 23.690168380737305\n","Acc: 43850/49984 Loss: 26.159801483154297\n","Train accuracy: 87.7281%\n","Test accuracy: 71.1600%\n","Epoch 109\n","Acc: 11353/12800 Loss: 24.355667114257812\n","Acc: 22637/25600 Loss: 21.07335090637207\n","Acc: 33905/38400 Loss: 23.50263786315918\n","Acc: 44041/49984 Loss: 22.50968360900879\n","Train accuracy: 88.1102%\n","Test accuracy: 71.1700%\n","Epoch 110\n","Acc: 11358/12800 Loss: 23.068523406982422\n","Acc: 22636/25600 Loss: 20.64725685119629\n","Acc: 33868/38400 Loss: 19.599586486816406\n","Acc: 43952/49984 Loss: 25.86625862121582\n","Train accuracy: 87.9321%\n","Test accuracy: 71.9800%\n","Epoch 111\n","Acc: 11383/12800 Loss: 22.930194854736328\n","Acc: 22626/25600 Loss: 21.761226654052734\n","Acc: 33836/38400 Loss: 22.753400802612305\n","Acc: 43924/49984 Loss: 22.787141799926758\n","Train accuracy: 87.8761%\n","Test accuracy: 70.7800%\n","Epoch 112\n","Acc: 11349/12800 Loss: 23.607364654541016\n","Acc: 22629/25600 Loss: 22.623397827148438\n","Acc: 33874/38400 Loss: 29.259740829467773\n","Acc: 44012/49984 Loss: 22.732572555541992\n","Train accuracy: 88.0522%\n","Test accuracy: 71.3400%\n","Epoch 113\n","Acc: 11328/12800 Loss: 21.736705780029297\n","Acc: 22599/25600 Loss: 26.232152938842773\n","Acc: 33898/38400 Loss: 23.93681526184082\n","Acc: 44088/49984 Loss: 21.204103469848633\n","Train accuracy: 88.2042%\n","Test accuracy: 71.2800%\n","Epoch 114\n","Acc: 11355/12800 Loss: 23.267236709594727\n","Acc: 22634/25600 Loss: 20.72041130065918\n","Acc: 33975/38400 Loss: 23.5583553314209\n","Acc: 44163/49984 Loss: 24.960142135620117\n","Train accuracy: 88.3543%\n","Test accuracy: 71.4700%\n","Epoch 115\n","Acc: 11360/12800 Loss: 24.97713279724121\n","Acc: 22721/25600 Loss: 22.37058448791504\n","Acc: 34024/38400 Loss: 22.90906524658203\n","Acc: 44154/49984 Loss: 21.63511085510254\n","Train accuracy: 88.3363%\n","Test accuracy: 71.4000%\n","Epoch 116\n","Acc: 11414/12800 Loss: 16.48939323425293\n","Acc: 22729/25600 Loss: 21.637266159057617\n","Acc: 33992/38400 Loss: 25.463014602661133\n","Acc: 44156/49984 Loss: 22.74418067932129\n","Train accuracy: 88.3403%\n","Test accuracy: 71.3100%\n","Epoch 117\n","Acc: 11429/12800 Loss: 23.244340896606445\n","Acc: 22763/25600 Loss: 25.62727928161621\n","Acc: 34042/38400 Loss: 21.160415649414062\n","Acc: 44224/49984 Loss: 22.727012634277344\n","Train accuracy: 88.4763%\n","Test accuracy: 71.5700%\n","Epoch 118\n","Acc: 11402/12800 Loss: 19.269649505615234\n","Acc: 22709/25600 Loss: 24.048572540283203\n","Acc: 33981/38400 Loss: 21.033050537109375\n","Acc: 44200/49984 Loss: 23.968965530395508\n","Train accuracy: 88.4283%\n","Test accuracy: 71.2800%\n","Epoch 119\n","Acc: 11421/12800 Loss: 22.54819107055664\n","Acc: 22761/25600 Loss: 24.28323745727539\n","Acc: 34080/38400 Loss: 22.710832595825195\n","Acc: 44224/49984 Loss: 19.025575637817383\n","Train accuracy: 88.4763%\n","Test accuracy: 71.4700%\n","Epoch 120\n","Acc: 11443/12800 Loss: 24.365009307861328\n","Acc: 22818/25600 Loss: 20.368391036987305\n","Acc: 34158/38400 Loss: 24.201332092285156\n","Acc: 44300/49984 Loss: 20.425580978393555\n","Train accuracy: 88.6284%\n","Test accuracy: 71.0200%\n","Epoch 121\n","Acc: 11432/12800 Loss: 24.209537506103516\n","Acc: 22770/25600 Loss: 21.464391708374023\n","Acc: 34090/38400 Loss: 20.684900283813477\n","Acc: 44280/49984 Loss: 22.323471069335938\n","Train accuracy: 88.5883%\n","Test accuracy: 71.1200%\n","Epoch 122\n","Acc: 11386/12800 Loss: 23.103395462036133\n","Acc: 22724/25600 Loss: 20.2264404296875\n","Acc: 34048/38400 Loss: 24.316837310791016\n","Acc: 44252/49984 Loss: 22.285245895385742\n","Train accuracy: 88.5323%\n","Test accuracy: 71.0200%\n","Epoch 123\n","Acc: 11388/12800 Loss: 22.255481719970703\n","Acc: 22749/25600 Loss: 25.486831665039062\n","Acc: 34083/38400 Loss: 20.88704490661621\n","Acc: 44363/49984 Loss: 25.18696403503418\n","Train accuracy: 88.7544%\n","Test accuracy: 71.6000%\n","Epoch 124\n","Acc: 11378/12800 Loss: 22.828189849853516\n","Acc: 22745/25600 Loss: 23.095050811767578\n","Acc: 34078/38400 Loss: 22.257678985595703\n","Acc: 44343/49984 Loss: 24.6383056640625\n","Train accuracy: 88.7144%\n","Test accuracy: 71.5700%\n","Epoch 125\n","Acc: 11463/12800 Loss: 21.931690216064453\n","Acc: 22847/25600 Loss: 23.734851837158203\n","Acc: 34154/38400 Loss: 23.64248275756836\n","Acc: 44381/49984 Loss: 20.995445251464844\n","Train accuracy: 88.7904%\n","Test accuracy: 71.6800%\n","Epoch 126\n","Acc: 11498/12800 Loss: 24.573610305786133\n","Acc: 22889/25600 Loss: 24.890134811401367\n","Acc: 34228/38400 Loss: 20.40186309814453\n","Acc: 44460/49984 Loss: 22.9224910736084\n","Train accuracy: 88.9485%\n","Test accuracy: 71.6100%\n","Epoch 127\n","Acc: 11527/12800 Loss: 19.87224578857422\n","Acc: 22893/25600 Loss: 24.47162437438965\n","Acc: 34235/38400 Loss: 23.536216735839844\n","Acc: 44476/49984 Loss: 22.779216766357422\n","Train accuracy: 88.9805%\n","Test accuracy: 72.0500%\n","Epoch 128\n","Acc: 11532/12800 Loss: 23.82801055908203\n","Acc: 22973/25600 Loss: 20.069908142089844\n","Acc: 34296/38400 Loss: 24.563034057617188\n","Acc: 44548/49984 Loss: 23.917648315429688\n","Train accuracy: 89.1245%\n","Test accuracy: 71.8400%\n","Epoch 129\n","Acc: 11521/12800 Loss: 22.020503997802734\n","Acc: 22926/25600 Loss: 24.063814163208008\n","Acc: 34296/38400 Loss: 26.663959503173828\n","Acc: 44533/49984 Loss: 24.61197280883789\n","Train accuracy: 89.0945%\n","Test accuracy: 71.4500%\n","Epoch 130\n","Acc: 11460/12800 Loss: 21.170764923095703\n","Acc: 22862/25600 Loss: 20.196739196777344\n","Acc: 34306/38400 Loss: 23.270998001098633\n","Acc: 44559/49984 Loss: 20.771953582763672\n","Train accuracy: 89.1465%\n","Test accuracy: 71.4400%\n","Epoch 131\n","Acc: 11570/12800 Loss: 18.5712947845459\n","Acc: 22981/25600 Loss: 21.190956115722656\n","Acc: 34351/38400 Loss: 22.306196212768555\n","Acc: 44584/49984 Loss: 23.211734771728516\n","Train accuracy: 89.1965%\n","Test accuracy: 71.9700%\n","Epoch 132\n","Acc: 11534/12800 Loss: 24.39124870300293\n","Acc: 22945/25600 Loss: 20.815025329589844\n","Acc: 34315/38400 Loss: 20.924930572509766\n","Acc: 44560/49984 Loss: 21.17741584777832\n","Train accuracy: 89.1485%\n","Test accuracy: 71.9400%\n","Epoch 133\n","Acc: 11476/12800 Loss: 22.750490188598633\n","Acc: 22948/25600 Loss: 23.076072692871094\n","Acc: 34325/38400 Loss: 21.200538635253906\n","Acc: 44590/49984 Loss: 20.04676055908203\n","Train accuracy: 89.2085%\n","Test accuracy: 71.6100%\n","Epoch 134\n","Acc: 11534/12800 Loss: 20.287269592285156\n","Acc: 22997/25600 Loss: 20.19096565246582\n","Acc: 34390/38400 Loss: 24.602737426757812\n","Acc: 44681/49984 Loss: 20.195480346679688\n","Train accuracy: 89.3906%\n","Test accuracy: 71.8400%\n","Epoch 135\n","Acc: 11592/12800 Loss: 21.43269920349121\n","Acc: 23044/25600 Loss: 19.57294464111328\n","Acc: 34446/38400 Loss: 22.19389533996582\n","Acc: 44743/49984 Loss: 22.412052154541016\n","Train accuracy: 89.5146%\n","Test accuracy: 71.6700%\n","Epoch 136\n","Acc: 11533/12800 Loss: 18.420351028442383\n","Acc: 23013/25600 Loss: 19.648693084716797\n","Acc: 34451/38400 Loss: 22.344968795776367\n","Acc: 44772/49984 Loss: 22.398691177368164\n","Train accuracy: 89.5727%\n","Test accuracy: 71.5400%\n","Epoch 137\n","Acc: 11459/12800 Loss: 23.64418601989746\n","Acc: 22922/25600 Loss: 22.745206832885742\n","Acc: 34437/38400 Loss: 22.38127899169922\n","Acc: 44752/49984 Loss: 20.25543975830078\n","Train accuracy: 89.5327%\n","Test accuracy: 71.3300%\n","Epoch 138\n","Acc: 11513/12800 Loss: 23.283151626586914\n","Acc: 22942/25600 Loss: 24.48414421081543\n","Acc: 34360/38400 Loss: 19.63224220275879\n","Acc: 44694/49984 Loss: 23.088356018066406\n","Train accuracy: 89.4166%\n","Test accuracy: 71.7300%\n","Epoch 139\n","Acc: 11540/12800 Loss: 22.0661563873291\n","Acc: 23000/25600 Loss: 25.441118240356445\n","Acc: 34421/38400 Loss: 22.45856285095215\n","Acc: 44754/49984 Loss: 23.77029800415039\n","Train accuracy: 89.5367%\n","Test accuracy: 71.8800%\n","Epoch 140\n","Acc: 11560/12800 Loss: 19.509471893310547\n","Acc: 23038/25600 Loss: 23.019519805908203\n","Acc: 34481/38400 Loss: 25.342052459716797\n","Acc: 44751/49984 Loss: 28.512422561645508\n","Train accuracy: 89.5306%\n","Test accuracy: 72.0400%\n","Epoch 141\n","Acc: 11544/12800 Loss: 21.76012420654297\n","Acc: 23044/25600 Loss: 19.771869659423828\n","Acc: 34525/38400 Loss: 27.499460220336914\n","Acc: 44870/49984 Loss: 23.634037017822266\n","Train accuracy: 89.7687%\n","Test accuracy: 72.0200%\n","Epoch 142\n","Acc: 11505/12800 Loss: 20.873321533203125\n","Acc: 23022/25600 Loss: 20.093807220458984\n","Acc: 34495/38400 Loss: 21.278406143188477\n","Acc: 44814/49984 Loss: 20.63445472717285\n","Train accuracy: 89.6567%\n","Test accuracy: 72.1500%\n","Epoch 143\n","Acc: 11572/12800 Loss: 21.978958129882812\n","Acc: 23163/25600 Loss: 20.21584129333496\n","Acc: 34599/38400 Loss: 24.421184539794922\n","Acc: 44897/49984 Loss: 20.846826553344727\n","Train accuracy: 89.8227%\n","Test accuracy: 71.7100%\n","Epoch 144\n","Acc: 11556/12800 Loss: 22.135936737060547\n","Acc: 23043/25600 Loss: 21.899747848510742\n","Acc: 34506/38400 Loss: 22.44341278076172\n","Acc: 44888/49984 Loss: 20.713064193725586\n","Train accuracy: 89.8047%\n","Test accuracy: 71.7700%\n","Epoch 145\n","Acc: 11597/12800 Loss: 19.44533348083496\n","Acc: 23102/25600 Loss: 22.309160232543945\n","Acc: 34608/38400 Loss: 18.735694885253906\n","Acc: 44947/49984 Loss: 22.992223739624023\n","Train accuracy: 89.9228%\n","Test accuracy: 71.1800%\n","Epoch 146\n","Acc: 11576/12800 Loss: 19.12088966369629\n","Acc: 23053/25600 Loss: 21.226240158081055\n","Acc: 34570/38400 Loss: 22.733837127685547\n","Acc: 44878/49984 Loss: 24.574296951293945\n","Train accuracy: 89.7847%\n","Test accuracy: 71.8400%\n","Epoch 147\n","Acc: 11622/12800 Loss: 19.340160369873047\n","Acc: 23108/25600 Loss: 18.691749572753906\n","Acc: 34631/38400 Loss: 21.03029441833496\n","Acc: 44969/49984 Loss: 21.092105865478516\n","Train accuracy: 89.9668%\n","Test accuracy: 72.1000%\n","Epoch 148\n","Acc: 11612/12800 Loss: 19.97882843017578\n","Acc: 23183/25600 Loss: 22.841115951538086\n","Acc: 34689/38400 Loss: 21.93302345275879\n","Acc: 45059/49984 Loss: 21.231016159057617\n","Train accuracy: 90.1468%\n","Test accuracy: 71.7600%\n","Epoch 149\n","Acc: 11618/12800 Loss: 19.960529327392578\n","Acc: 23062/25600 Loss: 22.131275177001953\n","Acc: 34499/38400 Loss: 24.516136169433594\n","Acc: 44848/49984 Loss: 25.340848922729492\n","Train accuracy: 89.7247%\n","Test accuracy: 71.3400%\n","Epoch 150\n","Acc: 11594/12800 Loss: 22.861940383911133\n","Acc: 23201/25600 Loss: 21.839567184448242\n","Acc: 34708/38400 Loss: 20.278831481933594\n","Acc: 45071/49984 Loss: 23.78460693359375\n","Train accuracy: 90.1709%\n","Test accuracy: 71.6700%\n","Epoch 151\n","Acc: 11621/12800 Loss: 21.990318298339844\n","Acc: 23182/25600 Loss: 20.581518173217773\n","Acc: 34680/38400 Loss: 23.663196563720703\n","Acc: 45093/49984 Loss: 21.61199188232422\n","Train accuracy: 90.2149%\n","Test accuracy: 71.8400%\n","Epoch 152\n","Acc: 11609/12800 Loss: 21.16805076599121\n","Acc: 23134/25600 Loss: 22.65856170654297\n","Acc: 34633/38400 Loss: 20.616853713989258\n","Acc: 45020/49984 Loss: 18.907678604125977\n","Train accuracy: 90.0688%\n","Test accuracy: 71.4600%\n","Epoch 153\n","Acc: 11632/12800 Loss: 18.321237564086914\n","Acc: 23228/25600 Loss: 19.75463104248047\n","Acc: 34770/38400 Loss: 19.748748779296875\n","Acc: 45148/49984 Loss: 22.936073303222656\n","Train accuracy: 90.3249%\n","Test accuracy: 71.2800%\n","Epoch 154\n","Acc: 11631/12800 Loss: 24.36595916748047\n","Acc: 23170/25600 Loss: 24.447248458862305\n","Acc: 34671/38400 Loss: 23.548961639404297\n","Acc: 45125/49984 Loss: 21.225427627563477\n","Train accuracy: 90.2789%\n","Test accuracy: 71.9400%\n","Epoch 155\n","Acc: 11648/12800 Loss: 19.18646812438965\n","Acc: 23243/25600 Loss: 24.484689712524414\n","Acc: 34782/38400 Loss: 20.130884170532227\n","Acc: 45183/49984 Loss: 19.966297149658203\n","Train accuracy: 90.3949%\n","Test accuracy: 72.2300%\n","Epoch 156\n","Acc: 11656/12800 Loss: 18.840866088867188\n","Acc: 23275/25600 Loss: 22.8741397857666\n","Acc: 34781/38400 Loss: 20.817852020263672\n","Acc: 45190/49984 Loss: 21.766036987304688\n","Train accuracy: 90.4089%\n","Test accuracy: 71.7600%\n","Epoch 157\n","Acc: 11630/12800 Loss: 19.59340476989746\n","Acc: 23175/25600 Loss: 18.471092224121094\n","Acc: 34670/38400 Loss: 23.235937118530273\n","Acc: 45096/49984 Loss: 21.016475677490234\n","Train accuracy: 90.2209%\n","Test accuracy: 71.7300%\n","Epoch 158\n","Acc: 11645/12800 Loss: 21.0521297454834\n","Acc: 23255/25600 Loss: 21.079038619995117\n","Acc: 34783/38400 Loss: 23.29073715209961\n","Acc: 45147/49984 Loss: 23.320947647094727\n","Train accuracy: 90.3229%\n","Test accuracy: 71.7600%\n","Epoch 159\n","Acc: 11660/12800 Loss: 21.558815002441406\n","Acc: 23213/25600 Loss: 19.597461700439453\n","Acc: 34766/38400 Loss: 21.875564575195312\n","Acc: 45191/49984 Loss: 22.621828079223633\n","Train accuracy: 90.4109%\n","Test accuracy: 71.6600%\n","Epoch 160\n","Acc: 11657/12800 Loss: 20.327682495117188\n","Acc: 23192/25600 Loss: 28.05527114868164\n","Acc: 34733/38400 Loss: 22.98511505126953\n","Acc: 45144/49984 Loss: 19.487102508544922\n","Train accuracy: 90.3169%\n","Test accuracy: 72.0800%\n","Epoch 161\n","Acc: 11656/12800 Loss: 22.588943481445312\n","Acc: 23237/25600 Loss: 23.6383056640625\n","Acc: 34805/38400 Loss: 24.50594711303711\n","Acc: 45256/49984 Loss: 21.77378273010254\n","Train accuracy: 90.5410%\n","Test accuracy: 71.1100%\n","Epoch 162\n","Acc: 11630/12800 Loss: 23.186113357543945\n","Acc: 23263/25600 Loss: 22.686246871948242\n","Acc: 34790/38400 Loss: 18.473678588867188\n","Acc: 45218/49984 Loss: 21.546140670776367\n","Train accuracy: 90.4649%\n","Test accuracy: 71.6700%\n","Epoch 163\n","Acc: 11706/12800 Loss: 22.613262176513672\n","Acc: 23312/25600 Loss: 21.08845329284668\n","Acc: 34855/38400 Loss: 21.98490333557129\n","Acc: 45253/49984 Loss: 21.6125545501709\n","Train accuracy: 90.5350%\n","Test accuracy: 71.5800%\n","Epoch 164\n","Acc: 11616/12800 Loss: 21.8646183013916\n","Acc: 23312/25600 Loss: 20.093984603881836\n","Acc: 34886/38400 Loss: 20.97734260559082\n","Acc: 45293/49984 Loss: 21.28784942626953\n","Train accuracy: 90.6150%\n","Test accuracy: 71.9100%\n","Epoch 165\n","Acc: 11695/12800 Loss: 23.146373748779297\n","Acc: 23258/25600 Loss: 22.520320892333984\n","Acc: 34849/38400 Loss: 21.788530349731445\n","Acc: 45223/49984 Loss: 19.84283447265625\n","Train accuracy: 90.4750%\n","Test accuracy: 71.5100%\n","Epoch 166\n","Acc: 11645/12800 Loss: 25.72679328918457\n","Acc: 23274/25600 Loss: 21.28668212890625\n","Acc: 34878/38400 Loss: 20.553970336914062\n","Acc: 45344/49984 Loss: 24.565914154052734\n","Train accuracy: 90.7170%\n","Test accuracy: 71.6100%\n","Epoch 167\n","Acc: 11676/12800 Loss: 22.516155242919922\n","Acc: 23297/25600 Loss: 21.893604278564453\n","Acc: 34942/38400 Loss: 21.76337242126465\n","Acc: 45362/49984 Loss: 21.606740951538086\n","Train accuracy: 90.7530%\n","Test accuracy: 71.8400%\n","Epoch 168\n","Acc: 11648/12800 Loss: 17.875959396362305\n","Acc: 23285/25600 Loss: 24.2587947845459\n","Acc: 34858/38400 Loss: 22.175804138183594\n","Acc: 45318/49984 Loss: 21.130220413208008\n","Train accuracy: 90.6650%\n","Test accuracy: 71.5400%\n","Epoch 169\n","Acc: 11665/12800 Loss: 20.76532554626465\n","Acc: 23328/25600 Loss: 24.310300827026367\n","Acc: 34908/38400 Loss: 24.694196701049805\n","Acc: 45303/49984 Loss: 22.089439392089844\n","Train accuracy: 90.6350%\n","Test accuracy: 71.8300%\n","Epoch 170\n","Acc: 11712/12800 Loss: 22.9411563873291\n","Acc: 23328/25600 Loss: 25.099769592285156\n","Acc: 34961/38400 Loss: 21.10550308227539\n","Acc: 45427/49984 Loss: 23.28160285949707\n","Train accuracy: 90.8831%\n","Test accuracy: 71.8900%\n","Epoch 171\n","Acc: 11698/12800 Loss: 19.471668243408203\n","Acc: 23357/25600 Loss: 18.594818115234375\n","Acc: 34957/38400 Loss: 18.62203598022461\n","Acc: 45432/49984 Loss: 21.093326568603516\n","Train accuracy: 90.8931%\n","Test accuracy: 72.0000%\n","Epoch 172\n","Acc: 11714/12800 Loss: 20.167030334472656\n","Acc: 23385/25600 Loss: 21.522188186645508\n","Acc: 34985/38400 Loss: 23.393815994262695\n","Acc: 45506/49984 Loss: 19.958927154541016\n","Train accuracy: 91.0411%\n","Test accuracy: 71.9000%\n","Epoch 173\n","Acc: 11668/12800 Loss: 24.342260360717773\n","Acc: 23309/25600 Loss: 18.886701583862305\n","Acc: 34905/38400 Loss: 19.348264694213867\n","Acc: 45371/49984 Loss: 23.83890151977539\n","Train accuracy: 90.7710%\n","Test accuracy: 71.6300%\n","Epoch 174\n","Acc: 11717/12800 Loss: 18.138931274414062\n","Acc: 23324/25600 Loss: 21.25537872314453\n","Acc: 34972/38400 Loss: 22.67337989807129\n","Acc: 45412/49984 Loss: 23.316123962402344\n","Train accuracy: 90.8531%\n","Test accuracy: 71.7900%\n","Epoch 175\n","Acc: 11712/12800 Loss: 19.797725677490234\n","Acc: 23371/25600 Loss: 23.15227699279785\n","Acc: 34937/38400 Loss: 24.23433494567871\n","Acc: 45490/49984 Loss: 20.10893440246582\n","Train accuracy: 91.0091%\n","Test accuracy: 71.7700%\n","Epoch 176\n","Acc: 11772/12800 Loss: 21.175884246826172\n","Acc: 23477/25600 Loss: 20.78540802001953\n","Acc: 35054/38400 Loss: 19.67107582092285\n","Acc: 45550/49984 Loss: 22.987857818603516\n","Train accuracy: 91.1292%\n","Test accuracy: 71.5400%\n","Epoch 177\n","Acc: 11772/12800 Loss: 21.15245819091797\n","Acc: 23451/25600 Loss: 18.47270965576172\n","Acc: 35039/38400 Loss: 18.96513557434082\n","Acc: 45516/49984 Loss: 20.340707778930664\n","Train accuracy: 91.0611%\n","Test accuracy: 71.5800%\n","Epoch 178\n","Acc: 11735/12800 Loss: 18.98557472229004\n","Acc: 23420/25600 Loss: 21.17759895324707\n","Acc: 35079/38400 Loss: 21.14104461669922\n","Acc: 45599/49984 Loss: 23.046558380126953\n","Train accuracy: 91.2272%\n","Test accuracy: 72.0100%\n","Epoch 179\n","Acc: 11781/12800 Loss: 18.256601333618164\n","Acc: 23458/25600 Loss: 19.063383102416992\n","Acc: 35067/38400 Loss: 20.900663375854492\n","Acc: 45543/49984 Loss: 23.698944091796875\n","Train accuracy: 91.1152%\n","Test accuracy: 72.0100%\n","Epoch 180\n","Acc: 11831/12800 Loss: 20.51934051513672\n","Acc: 23758/25600 Loss: 16.89024543762207\n","Acc: 35683/38400 Loss: 19.079795837402344\n","Acc: 46461/49984 Loss: 21.84334945678711\n","Train accuracy: 92.9517%\n","Test accuracy: 73.1500%\n","Epoch 181\n","Acc: 11966/12800 Loss: 19.31679344177246\n","Acc: 23940/25600 Loss: 19.807838439941406\n","Acc: 35905/38400 Loss: 19.005615234375\n","Acc: 46683/49984 Loss: 18.34101676940918\n","Train accuracy: 93.3959%\n","Test accuracy: 73.3600%\n","Epoch 182\n","Acc: 11957/12800 Loss: 18.513111114501953\n","Acc: 23906/25600 Loss: 18.759567260742188\n","Acc: 35900/38400 Loss: 17.581777572631836\n","Acc: 46788/49984 Loss: 17.76679229736328\n","Train accuracy: 93.6060%\n","Test accuracy: 73.2600%\n","Epoch 183\n","Acc: 12012/12800 Loss: 18.50298500061035\n","Acc: 23936/25600 Loss: 19.902339935302734\n","Acc: 35952/38400 Loss: 19.091768264770508\n","Acc: 46826/49984 Loss: 18.731666564941406\n","Train accuracy: 93.6820%\n","Test accuracy: 73.2800%\n","Epoch 184\n","Acc: 11995/12800 Loss: 17.399953842163086\n","Acc: 24018/25600 Loss: 22.153539657592773\n","Acc: 36033/38400 Loss: 18.85616683959961\n","Acc: 46934/49984 Loss: 17.11222267150879\n","Train accuracy: 93.8980%\n","Test accuracy: 73.3100%\n","Epoch 185\n","Acc: 12061/12800 Loss: 18.102201461791992\n","Acc: 24015/25600 Loss: 17.234722137451172\n","Acc: 35983/38400 Loss: 19.6025447845459\n","Acc: 46840/49984 Loss: 18.323955535888672\n","Train accuracy: 93.7100%\n","Test accuracy: 73.1800%\n","Epoch 186\n","Acc: 11972/12800 Loss: 17.00216293334961\n","Acc: 24007/25600 Loss: 20.286922454833984\n","Acc: 35978/38400 Loss: 17.832168579101562\n","Acc: 46843/49984 Loss: 18.99334716796875\n","Train accuracy: 93.7160%\n","Test accuracy: 73.3100%\n","Epoch 187\n","Acc: 12007/12800 Loss: 15.472334861755371\n","Acc: 24055/25600 Loss: 17.65557098388672\n","Acc: 36042/38400 Loss: 17.411163330078125\n","Acc: 46919/49984 Loss: 15.401596069335938\n","Train accuracy: 93.8680%\n","Test accuracy: 73.3800%\n","Epoch 188\n","Acc: 12087/12800 Loss: 18.996200561523438\n","Acc: 24132/25600 Loss: 15.777935028076172\n","Acc: 36116/38400 Loss: 17.0494327545166\n","Acc: 46949/49984 Loss: 18.413928985595703\n","Train accuracy: 93.9281%\n","Test accuracy: 73.4600%\n","Epoch 189\n","Acc: 12033/12800 Loss: 18.127822875976562\n","Acc: 24087/25600 Loss: 19.51279067993164\n","Acc: 36102/38400 Loss: 16.14708709716797\n","Acc: 46983/49984 Loss: 15.817815780639648\n","Train accuracy: 93.9961%\n","Test accuracy: 73.3500%\n","Epoch 190\n","Acc: 12069/12800 Loss: 17.244491577148438\n","Acc: 24125/25600 Loss: 19.947660446166992\n","Acc: 36177/38400 Loss: 17.12798309326172\n","Acc: 47091/49984 Loss: 16.692190170288086\n","Train accuracy: 94.2121%\n","Test accuracy: 73.2600%\n","Epoch 191\n","Acc: 12012/12800 Loss: 14.0353364944458\n","Acc: 24001/25600 Loss: 21.63169288635254\n","Acc: 36048/38400 Loss: 18.072402954101562\n","Acc: 46976/49984 Loss: 19.137451171875\n","Train accuracy: 93.9821%\n","Test accuracy: 73.3800%\n","Epoch 192\n","Acc: 12052/12800 Loss: 17.866628646850586\n","Acc: 24094/25600 Loss: 17.87578582763672\n","Acc: 36162/38400 Loss: 17.905622482299805\n","Acc: 47030/49984 Loss: 21.187395095825195\n","Train accuracy: 94.0901%\n","Test accuracy: 73.4400%\n","Epoch 193\n","Acc: 12050/12800 Loss: 16.153064727783203\n","Acc: 24141/25600 Loss: 19.19967269897461\n","Acc: 36164/38400 Loss: 19.470163345336914\n","Acc: 47034/49984 Loss: 17.435083389282227\n","Train accuracy: 94.0981%\n","Test accuracy: 73.4300%\n","Epoch 194\n","Acc: 12083/12800 Loss: 19.03279685974121\n","Acc: 24116/25600 Loss: 18.734333038330078\n","Acc: 36173/38400 Loss: 16.74891471862793\n","Acc: 47089/49984 Loss: 15.852046966552734\n","Train accuracy: 94.2081%\n","Test accuracy: 73.3200%\n","Epoch 195\n","Acc: 12081/12800 Loss: 17.522794723510742\n","Acc: 24094/25600 Loss: 16.22869110107422\n","Acc: 36182/38400 Loss: 17.778623580932617\n","Acc: 47043/49984 Loss: 21.358728408813477\n","Train accuracy: 94.1161%\n","Test accuracy: 73.2400%\n","Epoch 196\n","Acc: 12076/12800 Loss: 19.576391220092773\n","Acc: 24147/25600 Loss: 15.40960693359375\n","Acc: 36170/38400 Loss: 18.11338996887207\n","Acc: 47036/49984 Loss: 19.364805221557617\n","Train accuracy: 94.1021%\n","Test accuracy: 73.4800%\n","Epoch 197\n","Acc: 12080/12800 Loss: 16.85364532470703\n","Acc: 24167/25600 Loss: 17.76802635192871\n","Acc: 36234/38400 Loss: 19.043399810791016\n","Acc: 47182/49984 Loss: 17.3310489654541\n","Train accuracy: 94.3942%\n","Test accuracy: 73.1900%\n","Epoch 198\n","Acc: 12085/12800 Loss: 17.90746307373047\n","Acc: 24158/25600 Loss: 18.745040893554688\n","Acc: 36240/38400 Loss: 13.920104026794434\n","Acc: 47156/49984 Loss: 16.295005798339844\n","Train accuracy: 94.3422%\n","Test accuracy: 73.3800%\n","Epoch 199\n","Acc: 12056/12800 Loss: 19.188899993896484\n","Acc: 24100/25600 Loss: 17.00532341003418\n","Acc: 36188/38400 Loss: 15.13655948638916\n","Acc: 47151/49984 Loss: 17.88324737548828\n","Train accuracy: 94.3322%\n","Test accuracy: 73.3800%\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["CKPT_PATH = \"./student_checkpoint.pth\"\n","\n","trainer = KDTrainer()\n","trainer.train_student()\n","trainer.save_student_checkpoint(CKPT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nLiMY0d2IgYx","outputId":"07261732-90d5-428f-99d9-8a2fc4236887"},"outputs":[{"name":"stdout","output_type":"stream","text":["Student model test accuracy: 73.380 %\n","Is above threshold performance? True\n"]}],"source":["trainer.load_student_checkpoint(CKPT_PATH)\n","accuracy = trainer.evaluate_student()\n","\n","print(f\"Student model test accuracy: {accuracy:.3f} %\")\n","print(f\"Is above threshold performance? {accuracy > 72.2}\")"]},{"cell_type":"markdown","metadata":{"id":"sFoXBraEIgYx"},"source":["# Improved KD algorithm implementation (optional)\n","\n","This section is optional -- you can decide to leave this section untouched.\n","\n","Here, you can implement a more advanced KD algorithm of your choice.\n","We will give bonus credits depending on the performance of your KD algorithm.\n","\n","You are not allowed to edit the model architecture and data augmentation method. If you need to access the hidden layer features, refer to [nn.Module hooks](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=forward_hook#torch.nn.Module.register_forward_hook)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G67UGhnUIgYx"},"outputs":[],"source":["### IMPLEMENT THIS TRAINER CLASS ###\n","\n","class ImprovedKDTrainer(BaseTrainer):\n","    def __init__(self):\n","        super().__init__()\n","        ### YOU MAY EDIT BELOW ###\n","        self.train_dataloader = DataLoader(self.train_set, batch_size=64, shuffle=True, num_workers=2, drop_last=True)\n","\n","        self.init_lr = 0.1\n","        self.max_epoch = 200\n","        self.optimizer = optim.SGD(self.student.parameters(), lr=self.init_lr, momentum=0.9, weight_decay=1e-4)\n","        ...\n","\n","    def train_student(self):\n","        #### IMPLEMENT TRAINING HERE ####\n","        self.teacher.cuda().eval()\n","        self.student.cuda().train()\n","        for epoch in range(self.max_epoch):\n","            ..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":633,"status":"ok","timestamp":1701261817499,"user":{"displayName":"이호준","userId":"02920364108939530246"},"user_tz":-540},"id":"ZBZ8haEjGs5B","outputId":"6572f3e6-e8a0-42a5-fd71-cc538151135a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYEj57jyIgYx"},"outputs":[],"source":["CKPT_PATH = \"./student_improved_checkpoint.pth\"\n","\n","trainer = ImprovedKDTrainer()\n","trainer.train_student()\n","trainer.save_student_checkpoint(CKPT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"651YRScHIgYy"},"outputs":[],"source":["trainer.load_student_checkpoint(CKPT_PATH)\n","accuracy = trainer.evaluate_student()\n","\n","print(f\"Improved student model test accuracy: {accuracy:.3f} %\")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":0}